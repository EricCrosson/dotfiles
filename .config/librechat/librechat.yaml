version: 1.0.8

endpoints:
  custom:
    - name: "LiteLLM"
      # A place holder - otherwise it becomes the default (OpenAI) key
      # Provide the key instead in each "model" block within "litellm/litellm-config.yaml"
      apiKey: "sk-from-config-file"
      # See the required changes above in "Start LiteLLM Proxy Server" step.
      baseURL: "http://host.docker.internal:4000"
      # A "default" model to start new users with. The "fetch" will pull the rest of the available models from LiteLLM
      # More or less this is "irrelevant", you can pick any model. Just pick one you have defined in LiteLLM.
      models:
        default: ["bedrock-claude-sonnet"]
        fetch: true
      titleConvo: true
      titleModel: "bedrock-claude-sonnet"
      summarize: false
      summaryModel: "bedrock-claude-sonnet"
      forcePrompt: false
      modelDisplayLabel: "LiteLLM"
